{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2479a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 169 CSV files.\n",
      "Total columns in union: 47\n",
      "[1/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00000-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[2/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00001-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[3/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00002-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[4/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00003-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[5/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00004-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[6/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00005-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[7/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00006-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[8/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00007-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[9/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00008-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[10/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00009-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[11/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00010-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[12/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00011-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[13/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00012-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[14/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00013-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[15/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00014-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[16/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00015-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[17/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00016-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[18/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00017-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[19/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00018-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[20/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00019-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[21/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00020-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[22/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00021-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[23/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00022-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[24/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00023-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[25/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00024-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[26/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00025-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[27/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00026-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[28/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00027-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[29/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00028-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[30/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00029-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[31/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00030-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[32/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00031-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[33/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00032-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[34/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00033-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[35/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00034-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[36/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00035-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[37/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00036-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[38/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00037-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[39/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00038-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[40/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00039-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[41/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00040-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[42/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00041-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[43/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00042-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[44/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00043-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[45/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00044-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[46/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00045-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[47/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00046-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[48/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00047-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[49/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00048-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[50/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00049-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[51/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00050-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[52/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00051-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[53/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00052-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[54/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00053-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[55/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00054-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[56/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00055-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[57/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00056-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[58/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00057-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[59/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00058-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[60/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00059-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[61/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00060-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[62/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00061-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[63/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00062-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[64/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00063-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[65/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00064-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[66/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00065-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[67/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00066-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[68/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00067-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[69/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00068-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[70/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00069-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[71/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00070-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[72/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00071-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[73/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00072-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[74/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00073-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[75/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00074-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[76/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00075-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[77/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00076-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[78/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00077-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[79/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00078-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[80/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00079-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[81/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00080-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[82/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00081-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[83/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00082-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[84/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00083-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[85/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00084-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[86/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00085-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[87/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00086-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[88/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00087-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[89/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00088-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[90/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00089-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[91/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00090-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[92/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00091-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[93/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00092-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[94/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00093-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[95/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00094-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[96/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00095-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[97/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00096-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[98/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00097-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[99/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00098-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[100/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00099-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[101/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00100-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[102/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00101-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[103/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00102-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[104/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00103-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[105/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00104-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[106/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00105-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[107/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00106-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[108/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00107-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[109/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00108-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[110/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00109-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[111/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00110-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[112/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00111-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[113/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00112-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[114/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00113-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[115/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00114-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[116/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00115-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[117/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00116-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[118/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00117-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[119/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00118-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[120/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00119-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[121/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00120-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[122/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00121-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[123/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00122-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[124/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00123-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[125/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00124-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[126/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00125-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[127/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00126-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[128/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00127-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[129/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00128-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[130/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00129-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[131/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00130-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[132/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00131-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[133/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00132-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[134/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00133-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[135/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00134-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[136/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00135-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[137/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00136-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[138/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00137-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[139/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00138-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[140/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00139-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[141/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00140-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[142/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00141-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[143/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00142-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[144/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00143-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[145/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00144-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[146/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00145-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[147/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00146-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[148/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00147-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[149/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00148-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[150/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00149-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[151/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00150-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[152/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00151-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[153/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00152-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[154/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00153-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[155/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00154-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[156/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00155-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[157/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00156-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[158/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00157-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[159/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00158-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[160/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00159-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[161/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00160-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[162/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00161-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[163/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00162-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[164/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00163-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[165/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00164-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[166/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00165-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[167/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00166-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[168/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00167-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "[169/169] Processing: D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\\part-00168-363d1ba3-8ab5-4f96-bc25-4d5862db7cb9-c000.csv\n",
      "\n",
      "[OK] Wrote 46686579 rows to D:/PARA/projects/FYP/IDS2/magnum_opus/combined_raw.csv\n",
      "Done (streaming merge, memory-safe).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# ==== USER SETTINGS ====\n",
    "INPUT_DIR = r\"D:/PARA/projects/FYP/IDS2/CIC_dataset/wataiData/csv/CICIoT2023\"   # folder where all your CSVs are\n",
    "OUTPUT_FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_raw.csv\"\n",
    "CHUNK_SIZE  = 500_000   # adjust if you want smaller/larger memory footprint\n",
    "# =======================\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# 1) Find CSVs\n",
    "csv_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"**\", \"*.csv\"), recursive=True))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found under: {INPUT_DIR}\")\n",
    "print(f\"Found {len(csv_files)} CSV files.\")\n",
    "\n",
    "# 2) Build the union of columns (without loading full data)\n",
    "union_cols = []\n",
    "seen = set()\n",
    "for fp in csv_files:\n",
    "    try:\n",
    "        hdr = pd.read_csv(fp, nrows=0, low_memory=False)\n",
    "        cols = [c.strip() for c in hdr.columns]\n",
    "        for c in cols:\n",
    "            if c not in seen:\n",
    "                seen.add(c)\n",
    "                union_cols.append(c)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Skipping unreadable header: {fp} -> {e}\")\n",
    "\n",
    "if not union_cols:\n",
    "    raise RuntimeError(\"Could not read any headers to determine columns.\")\n",
    "\n",
    "print(f\"Total columns in union: {len(union_cols)}\")\n",
    "\n",
    "# 3) Stream through files chunk-by-chunk and append to one CSV\n",
    "wrote_header = False\n",
    "rows_written = 0\n",
    "\n",
    "for idx, fp in enumerate(csv_files, 1):\n",
    "    print(f\"[{idx}/{len(csv_files)}] Processing: {fp}\")\n",
    "    try:\n",
    "        # Use chunksize to avoid loading whole file\n",
    "        for chunk in pd.read_csv(fp, chunksize=CHUNK_SIZE, low_memory=False):\n",
    "            # Normalize column names\n",
    "            chunk.columns = [c.strip() for c in chunk.columns]\n",
    "            # Reindex to union columns (missing -> NaN); extra cols are dropped\n",
    "            chunk = chunk.reindex(columns=union_cols)\n",
    "            # Append to output\n",
    "            chunk.to_csv(OUTPUT_FILE, mode='a', index=False, header=not wrote_header)\n",
    "            wrote_header = True\n",
    "            rows_written += len(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Skipping file due to read error: {fp} -> {e}\")\n",
    "\n",
    "print(f\"\\n[OK] Wrote {rows_written} rows to {OUTPUT_FILE}\")\n",
    "print(\"Done (streaming merge, memory-safe).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f399be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD:\n",
      "   flow_duration  Header_Length  Protocol Type  Duration         Rate  \\\n",
      "0       0.000000          54.00           6.00     64.00     0.329807   \n",
      "1       0.000000          57.04           6.33     64.00     4.290556   \n",
      "2       0.000000           0.00           1.00     64.00    33.396799   \n",
      "3       0.328175       76175.00          17.00     64.00  4642.133010   \n",
      "4       0.117320         101.73           6.11     65.91     6.202211   \n",
      "\n",
      "         Srate  Drate  fin_flag_number  syn_flag_number  rst_flag_number  ...  \\\n",
      "0     0.329807    0.0              1.0              0.0              1.0  ...   \n",
      "1     4.290556    0.0              0.0              0.0              0.0  ...   \n",
      "2    33.396799    0.0              0.0              0.0              0.0  ...   \n",
      "3  4642.133010    0.0              0.0              0.0              0.0  ...   \n",
      "4     6.202211    0.0              0.0              1.0              0.0  ...   \n",
      "\n",
      "         Std  Tot size           IAT  Number   Magnitue     Radius  \\\n",
      "0   0.000000     54.00  8.334383e+07     9.5  10.392305   0.000000   \n",
      "1   2.822973     57.04  8.292607e+07     9.5  10.464666   4.010353   \n",
      "2   0.000000     42.00  8.312799e+07     9.5   9.165151   0.000000   \n",
      "3   0.000000     50.00  8.301570e+07     9.5  10.000000   0.000000   \n",
      "4  23.113111     57.88  8.297300e+07     9.5  11.346876  32.716243   \n",
      "\n",
      "    Covariance  Variance  Weight             label  \n",
      "0     0.000000      0.00  141.55  DDoS-RSTFINFlood  \n",
      "1   160.987842      0.05  141.55     DoS-TCP_Flood  \n",
      "2     0.000000      0.00  141.55   DDoS-ICMP_Flood  \n",
      "3     0.000000      0.00  141.55     DoS-UDP_Flood  \n",
      "4  3016.808286      0.19  141.55     DoS-SYN_Flood  \n",
      "\n",
      "[5 rows x 47 columns]\n",
      "\n",
      "Columns: ['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label']\n",
      "\n",
      "Total columns: 47\n",
      "\n",
      "Dtypes (quick):\n",
      "flow_duration      float64\n",
      "Header_Length      float64\n",
      "Protocol Type      float64\n",
      "Duration           float64\n",
      "Rate               float64\n",
      "Srate              float64\n",
      "Drate              float64\n",
      "fin_flag_number    float64\n",
      "syn_flag_number    float64\n",
      "rst_flag_number    float64\n",
      "psh_flag_number    float64\n",
      "ack_flag_number    float64\n",
      "ece_flag_number    float64\n",
      "cwr_flag_number    float64\n",
      "ack_count          float64\n",
      "syn_count          float64\n",
      "fin_count          float64\n",
      "urg_count          float64\n",
      "rst_count          float64\n",
      "HTTP               float64\n",
      "HTTPS              float64\n",
      "DNS                float64\n",
      "Telnet             float64\n",
      "SMTP               float64\n",
      "SSH                float64\n",
      "IRC                float64\n",
      "TCP                float64\n",
      "UDP                float64\n",
      "DHCP               float64\n",
      "ARP                float64\n",
      "ICMP               float64\n",
      "IPv                float64\n",
      "LLC                float64\n",
      "Tot sum            float64\n",
      "Min                float64\n",
      "Max                float64\n",
      "AVG                float64\n",
      "Std                float64\n",
      "Tot size           float64\n",
      "IAT                float64\n",
      "Number             float64\n",
      "Magnitue           float64\n",
      "Radius             float64\n",
      "Covariance         float64\n",
      "Variance           float64\n",
      "Weight             float64\n",
      "label               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_raw.csv\"\n",
    "\n",
    "# 1) Peek first few rows\n",
    "df_head = pd.read_csv(FILE, nrows=5)\n",
    "print(\"HEAD:\")\n",
    "print(df_head)\n",
    "\n",
    "# 2) Peek column names\n",
    "cols = list(df_head.columns)\n",
    "print(\"\\nColumns:\", cols)\n",
    "print(\"\\nTotal columns:\", len(cols))\n",
    "\n",
    "# 3) Peek dtypes (from first 5 rows only)\n",
    "print(\"\\nDtypes (quick):\")\n",
    "print(df_head.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a8abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected label column: label\n"
     ]
    }
   ],
   "source": [
    "possible_labels = {\"label\",\"Label\",\"attack\",\"Attack\",\"class\",\"Class\",\"target\",\"Target\",\"y\",\n",
    "                   \"Attack_type\",\"attack_type\",\"subcategory\",\"Category\",\"category\"}\n",
    "\n",
    "label_col = None\n",
    "for c in cols:\n",
    "    if c in possible_labels:\n",
    "        label_col = c\n",
    "\n",
    "print(\"Detected label column:\", label_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867c72dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CLASS DISTRIBUTION ===\n",
      "DDoS-ICMP_Flood: 7200504\n",
      "DDoS-UDP_Flood: 5412287\n",
      "DDoS-TCP_Flood: 4497667\n",
      "DDoS-PSHACK_Flood: 4094755\n",
      "DDoS-SYN_Flood: 4059190\n",
      "DDoS-RSTFINFlood: 4045285\n",
      "DDoS-SynonymousIP_Flood: 3598138\n",
      "DoS-UDP_Flood: 3318595\n",
      "DoS-TCP_Flood: 2671445\n",
      "DoS-SYN_Flood: 2028834\n",
      "BenignTraffic: 1098195\n",
      "Mirai-greeth_flood: 991866\n",
      "Mirai-udpplain: 890576\n",
      "Mirai-greip_flood: 751682\n",
      "DDoS-ICMP_Fragmentation: 452489\n",
      "MITM-ArpSpoofing: 307593\n",
      "DDoS-UDP_Fragmentation: 286925\n",
      "DDoS-ACK_Fragmentation: 285104\n",
      "DNS_Spoofing: 178911\n",
      "Recon-HostDiscovery: 134378\n",
      "Recon-OSScan: 98259\n",
      "Recon-PortScan: 82284\n",
      "DoS-HTTP_Flood: 71864\n",
      "VulnerabilityScan: 37382\n",
      "DDoS-HTTP_Flood: 28790\n",
      "DDoS-SlowLoris: 23426\n",
      "DictionaryBruteForce: 13064\n",
      "BrowserHijacking: 5859\n",
      "CommandInjection: 5409\n",
      "SqlInjection: 5245\n",
      "XSS: 3846\n",
      "Backdoor_Malware: 3218\n",
      "Recon-PingSweep: 2262\n",
      "Uploading_Attack: 1252\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_raw.csv\"\n",
    "CHUNK = 500_000\n",
    "\n",
    "label_counts = Counter()\n",
    "\n",
    "for chunk in pd.read_csv(FILE, usecols=[\"label\"], chunksize=CHUNK):\n",
    "    chunk[\"label\"] = chunk[\"label\"].astype(str)\n",
    "    label_counts.update(chunk[\"label\"])\n",
    "\n",
    "print(\"\\n=== CLASS DISTRIBUTION ===\")\n",
    "for k, v in label_counts.most_common():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62dda726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COLUMN LIST ===\n",
      "['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label']\n",
      "\n",
      "Total columns: 47\n",
      "\n",
      "=== DTYPES ===\n",
      "flow_duration      float64\n",
      "Header_Length      float64\n",
      "Protocol Type      float64\n",
      "Duration           float64\n",
      "Rate               float64\n",
      "Srate              float64\n",
      "Drate              float64\n",
      "fin_flag_number    float64\n",
      "syn_flag_number    float64\n",
      "rst_flag_number    float64\n",
      "psh_flag_number    float64\n",
      "ack_flag_number    float64\n",
      "ece_flag_number    float64\n",
      "cwr_flag_number    float64\n",
      "ack_count          float64\n",
      "syn_count          float64\n",
      "fin_count          float64\n",
      "urg_count          float64\n",
      "rst_count          float64\n",
      "HTTP               float64\n",
      "HTTPS              float64\n",
      "DNS                float64\n",
      "Telnet             float64\n",
      "SMTP               float64\n",
      "SSH                float64\n",
      "IRC                float64\n",
      "TCP                float64\n",
      "UDP                float64\n",
      "DHCP               float64\n",
      "ARP                float64\n",
      "ICMP               float64\n",
      "IPv                float64\n",
      "LLC                float64\n",
      "Tot sum            float64\n",
      "Min                float64\n",
      "Max                float64\n",
      "AVG                float64\n",
      "Std                float64\n",
      "Tot size           float64\n",
      "IAT                float64\n",
      "Number             float64\n",
      "Magnitue           float64\n",
      "Radius             float64\n",
      "Covariance         float64\n",
      "Variance           float64\n",
      "Weight             float64\n",
      "label               object\n",
      "dtype: object\n",
      "\n",
      "=== SAMPLE ROWS ===\n",
      "   flow_duration  Header_Length  Protocol Type  Duration         Rate  \\\n",
      "0       0.000000          54.00           6.00     64.00     0.329807   \n",
      "1       0.000000          57.04           6.33     64.00     4.290556   \n",
      "2       0.000000           0.00           1.00     64.00    33.396799   \n",
      "3       0.328175       76175.00          17.00     64.00  4642.133010   \n",
      "4       0.117320         101.73           6.11     65.91     6.202211   \n",
      "\n",
      "         Srate  Drate  fin_flag_number  syn_flag_number  rst_flag_number  ...  \\\n",
      "0     0.329807    0.0              1.0              0.0              1.0  ...   \n",
      "1     4.290556    0.0              0.0              0.0              0.0  ...   \n",
      "2    33.396799    0.0              0.0              0.0              0.0  ...   \n",
      "3  4642.133010    0.0              0.0              0.0              0.0  ...   \n",
      "4     6.202211    0.0              0.0              1.0              0.0  ...   \n",
      "\n",
      "         Std  Tot size           IAT  Number   Magnitue     Radius  \\\n",
      "0   0.000000     54.00  8.334383e+07     9.5  10.392305   0.000000   \n",
      "1   2.822973     57.04  8.292607e+07     9.5  10.464666   4.010353   \n",
      "2   0.000000     42.00  8.312799e+07     9.5   9.165151   0.000000   \n",
      "3   0.000000     50.00  8.301570e+07     9.5  10.000000   0.000000   \n",
      "4  23.113111     57.88  8.297300e+07     9.5  11.346876  32.716243   \n",
      "\n",
      "    Covariance  Variance  Weight             label  \n",
      "0     0.000000      0.00  141.55  DDoS-RSTFINFlood  \n",
      "1   160.987842      0.05  141.55     DoS-TCP_Flood  \n",
      "2     0.000000      0.00  141.55   DDoS-ICMP_Flood  \n",
      "3     0.000000      0.00  141.55     DoS-UDP_Flood  \n",
      "4  3016.808286      0.19  141.55     DoS-SYN_Flood  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_raw.csv\"\n",
    "\n",
    "# Only read a small sample (no memory issues)\n",
    "df = pd.read_csv(FILE, nrows=5000, low_memory=False)\n",
    "\n",
    "# Show columns\n",
    "print(\"\\n=== COLUMN LIST ===\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nTotal columns:\", len(df.columns))\n",
    "\n",
    "# Show dtypes\n",
    "print(\"\\n=== DTYPES ===\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Show first 5 rows\n",
    "print(\"\\n=== SAMPLE ROWS ===\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b41e4093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNIQUE VALUES PER COLUMN (top problematic) ===\n",
      "ece_flag_number    1\n",
      "cwr_flag_number    1\n",
      "SMTP               1\n",
      "Telnet             1\n",
      "IRC                1\n",
      "DHCP               1\n",
      "fin_flag_number    2\n",
      "syn_flag_number    2\n",
      "DNS                2\n",
      "HTTP               2\n",
      "TCP                2\n",
      "HTTPS              2\n",
      "rst_flag_number    2\n",
      "ack_flag_number    2\n",
      "psh_flag_number    2\n",
      "dtype: int64\n",
      "\n",
      "=== LOW VARIANCE COLUMNS ===\n",
      "ece_flag_number    0.000000e+00\n",
      "cwr_flag_number    0.000000e+00\n",
      "SMTP               0.000000e+00\n",
      "Telnet             0.000000e+00\n",
      "IRC                0.000000e+00\n",
      "DHCP               0.000000e+00\n",
      "Drate              1.191480e-07\n",
      "SSH                4.999775e-05\n",
      "ARP                6.999545e-05\n",
      "IPv                1.199862e-04\n",
      "LLC                1.199862e-04\n",
      "DNS                1.799685e-04\n",
      "HTTP               4.660374e-02\n",
      "HTTPS              5.209539e-02\n",
      "Variance           5.450901e-02\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_raw.csv\"\n",
    "\n",
    "df = pd.read_csv(FILE, nrows=200000)  # sample safely\n",
    "df = df.drop(columns=['label'])\n",
    "\n",
    "# replace inf with NaN\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# calculate number of unique values per column\n",
    "uniq = df.nunique()\n",
    "\n",
    "print(\"=== UNIQUE VALUES PER COLUMN (top problematic) ===\")\n",
    "print(uniq.sort_values().head(15))\n",
    "\n",
    "# calculate variance\n",
    "variance = df.var(numeric_only=True)\n",
    "\n",
    "print(\"\\n=== LOW VARIANCE COLUMNS ===\")\n",
    "print(variance.sort_values().head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c78f4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   flow_duration  Header_Length  Protocol Type  Duration         Rate  \\\n",
      "0       0.000000          54.00           6.00     64.00     0.329807   \n",
      "1       0.000000          57.04           6.33     64.00     4.290556   \n",
      "2       0.000000           0.00           1.00     64.00    33.396799   \n",
      "3       0.328175       76175.00          17.00     64.00  4642.133010   \n",
      "4       0.117320         101.73           6.11     65.91     6.202211   \n",
      "\n",
      "         Srate  Drate  fin_flag_number  syn_flag_number  rst_flag_number  ...  \\\n",
      "0     0.329807    0.0              1.0              0.0              1.0  ...   \n",
      "1     4.290556    0.0              0.0              0.0              0.0  ...   \n",
      "2    33.396799    0.0              0.0              0.0              0.0  ...   \n",
      "3  4642.133010    0.0              0.0              0.0              0.0  ...   \n",
      "4     6.202211    0.0              0.0              1.0              0.0  ...   \n",
      "\n",
      "         Std  Tot size           IAT  Number   Magnitue     Radius  \\\n",
      "0   0.000000     54.00  8.334383e+07     9.5  10.392305   0.000000   \n",
      "1   2.822973     57.04  8.292607e+07     9.5  10.464666   4.010353   \n",
      "2   0.000000     42.00  8.312799e+07     9.5   9.165151   0.000000   \n",
      "3   0.000000     50.00  8.301570e+07     9.5  10.000000   0.000000   \n",
      "4  23.113111     57.88  8.297300e+07     9.5  11.346876  32.716243   \n",
      "\n",
      "    Covariance  Variance  Weight             label  \n",
      "0     0.000000      0.00  141.55  DDoS-RSTFINFlood  \n",
      "1   160.987842      0.05  141.55     DoS-TCP_Flood  \n",
      "2     0.000000      0.00  141.55   DDoS-ICMP_Flood  \n",
      "3     0.000000      0.00  141.55     DoS-UDP_Flood  \n",
      "4  3016.808286      0.19  141.55     DoS-SYN_Flood  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "Columns AFTER cleaning: 41\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv(FILE, nrows=200000, low_memory=False)\n",
    "\n",
    "# drop the 6 columns\n",
    "df_full = df_full.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# handle inf → nan\n",
    "df_full = df_full.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# fill missing values\n",
    "df_full = df_full.fillna(0)\n",
    "\n",
    "print(df_full.head())\n",
    "print(\"Columns AFTER cleaning:\", len(df_full.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d784a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Final cleaned dataset created.\n",
      "Saved to: D:/PARA/projects/FYP/IDS2/magnum_opus/combined_clean_binary.csv.gz\n",
      "Total rows: 46,686,579\n",
      "Benign (0): 1,098,195   |   Attack (1): 45,588,384\n",
      "Total columns written: 42\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# === paths ===\n",
    "FILE     = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_raw.csv\"\n",
    "OUT_FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_clean_binary.csv.gz\"  # final output\n",
    "\n",
    "# === settings ===\n",
    "CHUNK = 500_000\n",
    "LABEL_COL = \"label\"\n",
    "BENIGN = \"BenignTraffic\"\n",
    "\n",
    "# columns to drop\n",
    "drop_cols = [\n",
    "    'ece_flag_number',\n",
    "    'cwr_flag_number',\n",
    "    'SMTP',\n",
    "    'Telnet',\n",
    "    'IRC',\n",
    "    'DHCP'\n",
    "]\n",
    "\n",
    "os.makedirs(os.path.dirname(OUT_FILE), exist_ok=True)\n",
    "\n",
    "wrote_header = False\n",
    "total_rows = 0\n",
    "benign_ct = 0\n",
    "attack_ct = 0\n",
    "kept_cols = None\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(FILE, chunksize=CHUNK, low_memory=False), 1):\n",
    "\n",
    "    # 1) drop 6 useless columns\n",
    "    chunk = chunk.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # 2) ensure label exists\n",
    "    if LABEL_COL not in chunk.columns:\n",
    "        raise ValueError(f\"'{LABEL_COL}' missing in chunk {i}.\")\n",
    "\n",
    "    # 3) clean values\n",
    "    chunk = chunk.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "    # 4) create binary label\n",
    "    chunk[LABEL_COL] = chunk[LABEL_COL].astype(str)\n",
    "    chunk[\"label_bin\"] = (chunk[LABEL_COL] != BENIGN).astype(\"int8\")\n",
    "\n",
    "    # 5) convert numeric columns → float32 (safe, reduces space)\n",
    "    numeric_cols = chunk.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if \"label_bin\" in numeric_cols:\n",
    "        numeric_cols.remove(\"label_bin\")\n",
    "    chunk[numeric_cols] = chunk[numeric_cols].astype(\"float32\")\n",
    "\n",
    "    # 6) lock column order on first chunk\n",
    "    if kept_cols is None:\n",
    "        kept_cols = list(chunk.columns)\n",
    "\n",
    "    out = chunk.reindex(columns=kept_cols)\n",
    "\n",
    "    # 7) stats\n",
    "    vc = out[\"label_bin\"].value_counts()\n",
    "    benign_ct += int(vc.get(0, 0))\n",
    "    attack_ct += int(vc.get(1, 0))\n",
    "    total_rows += len(out)\n",
    "\n",
    "    # 8) write final CSV (compressed)\n",
    "    out.to_csv(\n",
    "        OUT_FILE,\n",
    "        index=False,\n",
    "        header=not wrote_header,\n",
    "        mode='a',\n",
    "        compression='gzip'\n",
    "    )\n",
    "    wrote_header = True\n",
    "\n",
    "print(\"\\n[OK] Final cleaned dataset created.\")\n",
    "print(f\"Saved to: {OUT_FILE}\")\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"Benign (0): {benign_ct:,}   |   Attack (1): {attack_ct:,}\")\n",
    "print(f\"Total columns written: {len(kept_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5922921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'SSH', 'TCP', 'UDP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label', 'label_bin']\n",
      "Total columns: 42\n",
      "   flow_duration  Header_Length  Protocol Type  Duration         Rate  \\\n",
      "0       0.000000          54.00           6.00     64.00     0.329807   \n",
      "1       0.000000          57.04           6.33     64.00     4.290556   \n",
      "2       0.000000           0.00           1.00     64.00    33.396800   \n",
      "3       0.328175       76175.00          17.00     64.00  4642.133000   \n",
      "4       0.117320         101.73           6.11     65.91     6.202211   \n",
      "\n",
      "         Srate  Drate  fin_flag_number  syn_flag_number  rst_flag_number  ...  \\\n",
      "0     0.329807    0.0              1.0              0.0              1.0  ...   \n",
      "1     4.290556    0.0              0.0              0.0              0.0  ...   \n",
      "2    33.396800    0.0              0.0              0.0              0.0  ...   \n",
      "3  4642.133000    0.0              0.0              0.0              0.0  ...   \n",
      "4     6.202211    0.0              0.0              1.0              0.0  ...   \n",
      "\n",
      "   Tot size         IAT  Number   Magnitue     Radius  Covariance  Variance  \\\n",
      "0     54.00  83343830.0     9.5  10.392304   0.000000     0.00000      0.00   \n",
      "1     57.04  82926060.0     9.5  10.464666   4.010353   160.98784      0.05   \n",
      "2     42.00  83127990.0     9.5   9.165152   0.000000     0.00000      0.00   \n",
      "3     50.00  83015700.0     9.5  10.000000   0.000000     0.00000      0.00   \n",
      "4     57.88  82973000.0     9.5  11.346876  32.716244  3016.80830      0.19   \n",
      "\n",
      "   Weight             label  label_bin  \n",
      "0  141.55  DDoS-RSTFINFlood          1  \n",
      "1  141.55     DoS-TCP_Flood          1  \n",
      "2  141.55   DDoS-ICMP_Flood          1  \n",
      "3  141.55     DoS-UDP_Flood          1  \n",
      "4  141.55     DoS-SYN_Flood          1  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_clean_binary.csv.gz\"\n",
    "\n",
    "df = pd.read_csv(FILE, nrows=5)\n",
    "print(df.columns.tolist())\n",
    "print(\"Total columns:\", len(df.columns))\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8061b7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow_duration      float64\n",
      "Header_Length      float64\n",
      "Protocol Type      float64\n",
      "Duration           float64\n",
      "Rate               float64\n",
      "Srate              float64\n",
      "Drate              float64\n",
      "fin_flag_number    float64\n",
      "syn_flag_number    float64\n",
      "rst_flag_number    float64\n",
      "psh_flag_number    float64\n",
      "ack_flag_number    float64\n",
      "ack_count          float64\n",
      "syn_count          float64\n",
      "fin_count          float64\n",
      "urg_count          float64\n",
      "rst_count          float64\n",
      "HTTP               float64\n",
      "HTTPS              float64\n",
      "DNS                float64\n",
      "SSH                float64\n",
      "TCP                float64\n",
      "UDP                float64\n",
      "ARP                float64\n",
      "ICMP               float64\n",
      "IPv                float64\n",
      "LLC                float64\n",
      "Tot sum            float64\n",
      "Min                float64\n",
      "Max                float64\n",
      "AVG                float64\n",
      "Std                float64\n",
      "Tot size           float64\n",
      "IAT                float64\n",
      "Number             float64\n",
      "Magnitue           float64\n",
      "Radius             float64\n",
      "Covariance         float64\n",
      "Variance           float64\n",
      "Weight             float64\n",
      "label               object\n",
      "label_bin            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f2c7608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null present: False\n",
      "Inf present: False\n",
      "\n",
      "Unique label_bin: [1]\n",
      "\n",
      "Binary class counts: Counter({1: 45588384, 0: 1098195})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_clean_binary.csv.gz\"\n",
    "CHUNK = 500000\n",
    "\n",
    "# --- CHECK NULL AND INF ---\n",
    "null_found = False\n",
    "inf_found = False\n",
    "\n",
    "for chunk in pd.read_csv(FILE, chunksize=CHUNK, low_memory=False):\n",
    "    if chunk.isna().any().any():\n",
    "        null_found = True\n",
    "        break\n",
    "    if np.isinf(chunk.select_dtypes(include=[float, int])).any().any():\n",
    "        inf_found = True\n",
    "        break\n",
    "\n",
    "print(\"\\nNull present:\", null_found)\n",
    "print(\"Inf present:\", inf_found)\n",
    "\n",
    "# --- CHECK LABEL_BIN UNIQUE ---\n",
    "df = pd.read_csv(FILE, nrows=10)\n",
    "print(\"\\nUnique label_bin:\", df[\"label_bin\"].unique())\n",
    "\n",
    "# --- CLASS COUNTS ---\n",
    "counter = Counter()\n",
    "for chunk in pd.read_csv(FILE, usecols=[\"label_bin\"], chunksize=CHUNK):\n",
    "    counter.update(chunk[\"label_bin\"])\n",
    "\n",
    "print(\"\\nBinary class counts:\", counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be9c27af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "  Benign total: 1,098,195 -> cap 1,000,000\n",
      "  Attack labels kept: 26 (target rows 2,255,069)\n",
      "  Dropped labels: 7 → ['Backdoor_Malware', 'BrowserHijacking', 'CommandInjection', 'Recon-PingSweep', 'SqlInjection', 'Uploading_Attack', 'XSS']\n",
      "\n",
      "Collected: 3,255,069 rows (Benign 1,000,000 + Attacks 2,255,069)\n",
      "Top kept attacks (label: kept/target): DDoS-RSTFINFlood:100000/100000, DoS-TCP_Flood:100000/100000, DDoS-ICMP_Flood:100000/100000, DoS-UDP_Flood:100000/100000, DoS-SYN_Flood:100000/100000, Mirai-greeth_flood:100000/100000, DDoS-SynonymousIP_Flood:100000/100000, Mirai-udpplain:100000/100000, DDoS-SYN_Flood:100000/100000, DDoS-PSHACK_Flood:100000/100000 ...\n",
      "\n",
      "Shapes: \n",
      "  X_train (2278548, 40)  y_train (2278548,) \n",
      "  X_val   (488260, 40)  y_val   (488260,) \n",
      "  X_test  (488261, 40)  y_test  (488261,)\n",
      "\n",
      "[OK] Saved -> D:/PARA/projects/FYP/IDS2/magnum_opus/binary_balanced_tensors.npz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_clean_binary.csv.gz\"\n",
    "CHUNK = 500_000\n",
    "SEED  = 42\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "# ---- config you approved ----\n",
    "BENIGN_NAME = \"BenignTraffic\"\n",
    "BENIGN_CAP  = 1_000_000\n",
    "ATTACK_CAP  = 100_000\n",
    "MIN_KEEP    = 500  # drop any attack class with < 500 samples\n",
    "\n",
    "# explicit drop-list (your list)\n",
    "DROP_ATTACKS = {\n",
    "    \"BrowserHijacking\",\"CommandInjection\",\"SqlInjection\",\"XSS\",\n",
    "    \"Backdoor_Malware\",\"Recon-PingSweep\",\"Uploading_Attack\"\n",
    "}\n",
    "\n",
    "# ========== PASS 1: count per attack label + benign ==========\n",
    "label_counts = Counter()\n",
    "for chunk in pd.read_csv(FILE, usecols=[\"label\"], chunksize=CHUNK, low_memory=False):\n",
    "    chunk[\"label\"] = chunk[\"label\"].astype(str)\n",
    "    label_counts.update(chunk[\"label\"])\n",
    "\n",
    "benign_total = label_counts.get(BENIGN_NAME, 0)\n",
    "# Build quotas\n",
    "attack_labels = [lab for lab,cnt in label_counts.items()\n",
    "                 if lab != BENIGN_NAME]\n",
    "\n",
    "quotas = {}  # per-label target to collect\n",
    "dropped_labels = set()\n",
    "\n",
    "for lab in attack_labels:\n",
    "    cnt = label_counts[lab]\n",
    "    if lab in DROP_ATTACKS or cnt < MIN_KEEP:\n",
    "        dropped_labels.add(lab)\n",
    "        continue\n",
    "    quotas[lab] = min(cnt, ATTACK_CAP)\n",
    "\n",
    "benign_quota = min(benign_total, BENIGN_CAP)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(f\"  Benign total: {benign_total:,} -> cap {benign_quota:,}\")\n",
    "kept_attack_total_target = sum(quotas.values())\n",
    "print(f\"  Attack labels kept: {len(quotas)} (target rows {kept_attack_total_target:,})\")\n",
    "print(f\"  Dropped labels: {len(dropped_labels)} → {sorted(list(dropped_labels))[:10]}{' ...' if len(dropped_labels)>10 else ''}\")\n",
    "\n",
    "# ========== PASS 2: collect rows to meet quotas ==========\n",
    "frames = []\n",
    "kept_per_label = defaultdict(int)\n",
    "kept_benign = 0\n",
    "\n",
    "for chunk in pd.read_csv(FILE, chunksize=CHUNK, low_memory=False):\n",
    "    chunk[\"label\"] = chunk[\"label\"].astype(str)\n",
    "\n",
    "    # Benign\n",
    "    if kept_benign < benign_quota:\n",
    "        ben = chunk[chunk[\"label\"] == BENIGN_NAME]\n",
    "        need_b = benign_quota - kept_benign\n",
    "        if not ben.empty and need_b > 0:\n",
    "            take_b = ben.iloc[:min(len(ben), need_b)]\n",
    "            frames.append(take_b)\n",
    "            kept_benign += len(take_b)\n",
    "\n",
    "    # Attacks (kept labels only)\n",
    "    att = chunk[chunk[\"label\"] != BENIGN_NAME]\n",
    "    if not att.empty:\n",
    "        # filter out dropped labels\n",
    "        att = att[att[\"label\"].isin(quotas.keys())]\n",
    "        if not att.empty:\n",
    "            # iterate by label to respect per-class caps\n",
    "            for lab, sub in att.groupby(\"label\"):\n",
    "                need = quotas[lab] - kept_per_label[lab]\n",
    "                if need <= 0:\n",
    "                    continue\n",
    "                take = sub.iloc[:min(len(sub), need)]\n",
    "                if len(take):\n",
    "                    frames.append(take)\n",
    "                    kept_per_label[lab] += len(take)\n",
    "\n",
    "    # stop early if all quotas satisfied\n",
    "    if kept_benign >= benign_quota and all(kept_per_label[l] >= quotas[l] for l in quotas):\n",
    "        break\n",
    "\n",
    "# Concatenate\n",
    "df_bal = pd.concat(frames, ignore_index=True)\n",
    "# Shuffle once\n",
    "df_bal = df_bal.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nCollected: {len(df_bal):,} rows \"\n",
    "      f\"(Benign {kept_benign:,} + Attacks {sum(kept_per_label.values()):,})\")\n",
    "print(\"Top kept attacks (label: kept/target):\",\n",
    "      \", \".join([f\"{lab}:{kept_per_label[lab]}/{quotas[lab]}\" for lab in list(quotas)[:10]]), \"...\")\n",
    "\n",
    "# ========== Build X/y ==========\n",
    "feature_cols = [c for c in df_bal.columns if c not in (\"label\",\"label_bin\")]\n",
    "X = df_bal[feature_cols].astype(\"float32\").values\n",
    "y = df_bal[\"label_bin\"].astype(\"int8\").values\n",
    "\n",
    "# ========== Stratified 70/15/15 split ==========\n",
    "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=SEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_tmp, y_tmp, test_size=0.50, stratify=y_tmp, random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"\\nShapes:\",\n",
    "      \"\\n  X_train\", X_train.shape, \" y_train\", y_train.shape,\n",
    "      \"\\n  X_val  \", X_val.shape,   \" y_val  \", y_val.shape,\n",
    "      \"\\n  X_test \", X_test.shape,  \" y_test \", y_test.shape)\n",
    "\n",
    "# ========== Fit scaler on TRAIN ONLY ==========\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)                 # train-only\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "\n",
    "# ========== Save tensors (single compact file) ==========\n",
    "OUT_NPZ = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/binary_balanced_tensors.npz\"\n",
    "np.savez_compressed(\n",
    "    OUT_NPZ,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_val=X_val,     y_val=y_val,\n",
    "    X_test=X_test,   y_test=y_test,\n",
    "    feature_names=np.array(feature_cols, dtype=object),\n",
    "    kept_per_label=np.array(sorted([(k, kept_per_label[k]) for k in kept_per_label], key=lambda x:x[0]), dtype=object),\n",
    "    dropped_labels=np.array(sorted(list(dropped_labels)), dtype=object),\n",
    "    seed=np.array([SEED])\n",
    ")\n",
    "print(f\"\\n[OK] Saved -> {OUT_NPZ}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78de5b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tensors saved: D:/PARA/projects/FYP/IDS2/magnum_opus/binary_balanced_tensors.npz \n",
      "  X_train (2278548, 40)  y_train (2278548,) \n",
      "  X_val   (488260, 40)  y_val   (488260,) \n",
      "  X_test  (488261, 40)  y_test  (488261,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure float32 for memory + DL frameworks\n",
    "X_train = X_train.astype(\"float32\", copy=False)\n",
    "X_val   = X_val.astype(\"float32\",   copy=False)\n",
    "X_test  = X_test.astype(\"float32\",  copy=False)\n",
    "\n",
    "# 1) Fit scaler on TRAIN only (no leakage)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# 2) Transform all splits\n",
    "X_train = scaler.transform(X_train).astype(\"float32\", copy=False)\n",
    "X_val   = scaler.transform(X_val).astype(\"float32\",   copy=False)\n",
    "X_test  = scaler.transform(X_test).astype(\"float32\",  copy=False)\n",
    "\n",
    "# 3) (Optional but recommended) Save one compact file for reuse\n",
    "OUT_NPZ = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/binary_balanced_tensors.npz\"\n",
    "np.savez_compressed(\n",
    "    OUT_NPZ,\n",
    "    X_train=X_train, y_train=y_train.astype(\"int8\"),\n",
    "    X_val=X_val,     y_val=y_val.astype(\"int8\"),\n",
    "    X_test=X_test,   y_test=y_test.astype(\"int8\"),\n",
    ")\n",
    "print(\"[OK] Tensors saved:\", OUT_NPZ,\n",
    "      \"\\n  X_train\", X_train.shape, \" y_train\", y_train.shape,\n",
    "      \"\\n  X_val  \", X_val.shape,   \" y_val  \", y_val.shape,\n",
    "      \"\\n  X_test \", X_test.shape,  \" y_test \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75f1e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\New_folder\\envs\\pytorch\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts (train): [700000, 1578548]\n",
      "[Epoch 01] loss=0.1168  val_f1_macro=0.9132  acc=0.9220\n",
      "Target reached (val macro-F1 ≥ 0.85). Stopping.\n",
      "\n",
      "[Done]\n",
      "Best val macro-F1: 0.9131790043404009\n",
      "Test metrics: {\n",
      "  \"acc\": 0.9226356395452432,\n",
      "  \"f1_macro\": 0.913821147299156,\n",
      "  \"f1_weighted\": 0.9244480652881837,\n",
      "  \"f1_per_class\": [\n",
      "    0.8862598913606418,\n",
      "    0.9413824032376702\n",
      "  ],\n",
      "  \"cm\": [\n",
      "    [\n",
      "      147167,\n",
      "      2833\n",
      "    ],\n",
      "    [\n",
      "      34941,\n",
      "      303320\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "Saved: D:/PARA/projects/FYP/IDS2/magnum_opus\\model_kronnet_binary.pt and D:/PARA/projects/FYP/IDS2/magnum_opus\\metrics_binary.json\n"
     ]
    }
   ],
   "source": [
    "# KronNet binary training on prebuilt tensors\n",
    "# - architecture: 40 -> 64 -> 32 -> (4 -> 2x2 -> left-mult 2x2) -> head(2)\n",
    "# - loss: 0.7 * Focal(gamma=2) + 0.3 * CrossEntropy, with Class-Balanced (CB) weights\n",
    "# - sampler: WeightedRandomSampler from original y_train\n",
    "# - scheduler: ReduceLROnPlateau on val macro-F1 (factor 0.5, patience 2, min_lr 1e-5)\n",
    "# - early stop: patience 8; fail-fast: <0.70 by epoch 8 abort; target-stop: >=0.85\n",
    "\n",
    "import os, json, math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ------------------- config -------------------\n",
    "NPZ_PATH   = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/binary_balanced_tensors.npz\"\n",
    "OUT_DIR    = r\"D:/PARA/projects/FYP/IDS2/magnum_opus\"\n",
    "SEED       = 42\n",
    "BATCH_SIZE = 4096\n",
    "LR         = 1e-3\n",
    "WD         = 1e-5\n",
    "MAX_EPOCHS = 50\n",
    "PATIENCE   = 8\n",
    "GAMMA_FOCAL= 2.0\n",
    "CB_BETA    = 0.999    # effective number of samples\n",
    "DEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------- utils -------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False  # allow speed\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def class_balanced_weights(y, beta=CB_BETA, num_classes=2):\n",
    "    # y: numpy array of ints in [0, C)\n",
    "    counts = np.bincount(y.astype(np.int64), minlength=num_classes)\n",
    "    eff_num = 1.0 - np.power(beta, counts)\n",
    "    w = (1.0 - beta) / np.maximum(eff_num, 1e-12)\n",
    "    w = w / np.sum(w) * num_classes  # normalize to mean ~1\n",
    "    return w.astype(np.float32), counts\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "    def forward(self, logits, target):\n",
    "        # target: (N,) int64; logits: (N,C)\n",
    "        ce = F.cross_entropy(logits, target, weight=self.weight, reduction='none')\n",
    "        pt = torch.softmax(logits, dim=1).gather(1, target.view(-1,1)).squeeze(1)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        if self.reduction == \"mean\":\n",
    "            return loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# ------------------- model -------------------\n",
    "class KronNetBinary(nn.Module):\n",
    "    def __init__(self, in_dim, p_drop=0.05):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.map4 = nn.Linear(32, 4)     # -> reshape to (2,2)\n",
    "        # learnable left 2x2 multiply\n",
    "        self.L = nn.Parameter(torch.eye(2, dtype=torch.float32))\n",
    "        self.head = nn.Linear(4, 2)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "\n",
    "        # init\n",
    "        for m in [self.fc1, self.fc2, self.map4, self.head]:\n",
    "            nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "        with torch.no_grad():\n",
    "            self.L.copy_(torch.tensor([[1.0,0.0],[0.0,1.0]]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.drop(F.relu(self.fc1(x)))\n",
    "        x = self.drop(F.relu(self.fc2(x)))\n",
    "        x = self.map4(x)           # (N,4)\n",
    "        M = x.view(-1, 2, 2)       # (N,2,2)\n",
    "        L = self.L                 # (2,2)\n",
    "        M2 = torch.matmul(L, M)    # left multiply -> (2,2)\n",
    "        z = M2.view(-1, 4)         # (N,4)\n",
    "        out = self.head(z)         # (N,2)\n",
    "        return out\n",
    "\n",
    "# ------------------- data -------------------\n",
    "set_seed(SEED)\n",
    "data = np.load(NPZ_PATH, allow_pickle=False)\n",
    "X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "X_val,   y_val   = data[\"X_val\"],   data[\"y_val\"]\n",
    "X_test,  y_test  = data[\"X_test\"],  data[\"y_test\"]\n",
    "in_dim = X_train.shape[1]\n",
    "\n",
    "# tensors\n",
    "Xtr = torch.from_numpy(X_train).to(torch.float32)\n",
    "ytr = torch.from_numpy(y_train).to(torch.long)\n",
    "Xv  = torch.from_numpy(X_val).to(torch.float32)\n",
    "yv  = torch.from_numpy(y_val).to(torch.long)\n",
    "Xte = torch.from_numpy(X_test).to(torch.float32)\n",
    "yte = torch.from_numpy(y_test).to(torch.long)\n",
    "\n",
    "# sampler from original y_train (no oversampled labels)\n",
    "# inverse-frequency weights per-sample\n",
    "counts = np.bincount(y_train, minlength=2)\n",
    "w_inv = 1.0 / np.maximum(counts, 1)\n",
    "sample_w = torch.where(ytr==0, torch.tensor(w_inv[0]), torch.tensor(w_inv[1])).float()\n",
    "sampler = WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr, ytr), batch_size=BATCH_SIZE, sampler=sampler, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "val_loader   = DataLoader(TensorDataset(Xv,  yv ), batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "test_loader  = DataLoader(TensorDataset(Xte, yte), batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "\n",
    "# ------------------- loss / opt / sched -------------------\n",
    "cb_weights_np, cls_counts = class_balanced_weights(y_train, beta=CB_BETA, num_classes=2)\n",
    "cb_weights = torch.tensor(cb_weights_np, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion_focal = FocalLoss(gamma=GAMMA_FOCAL, weight=cb_weights)\n",
    "criterion_ce    = nn.CrossEntropyLoss(weight=cb_weights)\n",
    "\n",
    "model = KronNetBinary(in_dim).to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"max\", factor=0.5, patience=2, min_lr=1e-5, verbose=True)\n",
    "\n",
    "# ------------------- train/eval helpers -------------------\n",
    "@torch.no_grad()\n",
    "def eval_loader(loader, mdl):\n",
    "    mdl.eval()\n",
    "    all_p, all_t = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "        logits = mdl(xb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_p.append(preds.cpu().numpy())\n",
    "        all_t.append(yb.cpu().numpy())\n",
    "    y_pred = np.concatenate(all_p)\n",
    "    y_true = np.concatenate(all_t)\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    f1_ma = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_wt = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    f1_pc = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    cm    = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    return {\"acc\":acc, \"f1_macro\":f1_ma, \"f1_weighted\":f1_wt, \"f1_per_class\":f1_pc.tolist(), \"cm\":cm.tolist()}\n",
    "\n",
    "def train_epoch(loader, mdl, opt):\n",
    "    mdl.train()\n",
    "    total = 0; loss_sum = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = mdl(xb)\n",
    "        # hybrid loss\n",
    "        lf = criterion_focal(logits, yb)\n",
    "        lce= criterion_ce(logits, yb)\n",
    "        loss = 0.7*lf + 0.3*lce\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_sum += loss.item() * xb.size(0)\n",
    "        total += xb.size(0)\n",
    "    return loss_sum / max(total,1)\n",
    "\n",
    "# ------------------- training loop -------------------\n",
    "best_f1 = -1.0\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"Class counts (train):\", cls_counts.tolist())\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    tr_loss = train_epoch(train_loader, model, opt)\n",
    "    val_metrics = eval_loader(val_loader, model)\n",
    "    sched.step(val_metrics[\"f1_macro\"])\n",
    "\n",
    "    curr_f1 = val_metrics[\"f1_macro\"]\n",
    "    print(f\"[Epoch {epoch:02d}] loss={tr_loss:.4f}  val_f1_macro={curr_f1:.4f}  acc={val_metrics['acc']:.4f}\")\n",
    "\n",
    "    # target reached?\n",
    "    if curr_f1 >= 0.85:\n",
    "        best_f1 = curr_f1\n",
    "        best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "        print(\"Target reached (val macro-F1 ≥ 0.85). Stopping.\")\n",
    "        break\n",
    "\n",
    "    # track best\n",
    "    if curr_f1 > best_f1:\n",
    "        best_f1 = curr_f1\n",
    "        best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    # fail-fast guard\n",
    "    if epoch == 8 and best_f1 < 0.70:\n",
    "        print(\"Fail-fast: best val macro-F1 < 0.70 by epoch 8. Aborting to revisit data/config.\")\n",
    "        break\n",
    "\n",
    "    # early stopping\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping (no improve {PATIENCE} epochs).\")\n",
    "        break\n",
    "\n",
    "# ------------------- save + final test -------------------\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "torch.save(model.state_dict(), os.path.join(OUT_DIR, \"model_kronnet_binary.pt\"))\n",
    "\n",
    "val_report = eval_loader(val_loader, model)\n",
    "test_report= eval_loader(test_loader, model)\n",
    "\n",
    "metrics = {\n",
    "    \"seed\": SEED,\n",
    "    \"device\": DEVICE,\n",
    "    \"best_val_macroF1\": best_f1,\n",
    "    \"val\": val_report,\n",
    "    \"test\": test_report,\n",
    "    \"train_counts\": cls_counts.tolist(),\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"metrics_binary.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n[Done]\")\n",
    "print(\"Best val macro-F1:\", best_f1)\n",
    "print(\"Test metrics:\", json.dumps(test_report, indent=2))\n",
    "print(\"Saved:\", os.path.join(OUT_DIR, \"model_kronnet_binary.pt\"),\n",
    "      \"and\", os.path.join(OUT_DIR, \"metrics_binary.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "459b0662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.937269206428529\n",
      "Macro-F1: 0.9292021575158557\n",
      "Weighted-F1: 0.938416740416477\n",
      "Per-class F1: [0.90530384 0.95310048]\n",
      "\n",
      "Confusion matrix:\n",
      " [[146408   3592]\n",
      " [ 27037 311224]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8441    0.9761    0.9053    150000\n",
      "           1     0.9886    0.9201    0.9531    338261\n",
      "\n",
      "    accuracy                         0.9373    488261\n",
      "   macro avg     0.9164    0.9481    0.9292    488261\n",
      "weighted avg     0.9442    0.9373    0.9384    488261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# model already loaded as \"model\"\n",
    "model.eval()\n",
    "\n",
    "# we recompute the test predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        y_true.append(yb.cpu().numpy())\n",
    "        y_pred.append(preds.cpu().numpy())\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Macro-F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "print(\"Weighted-F1:\", f1_score(y_true, y_pred, average=\"weighted\"))\n",
    "print(\"Per-class F1:\", f1_score(y_true, y_pred, average=None))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bba85839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN METRICS ===\n",
      "Accuracy: 0.9477294311991672\n",
      "Macro-F1: 0.9476884373735617\n",
      "Weighted-F1: 0.9476878306765111\n",
      "Per-class F1: [0.94915283 0.94622404]\n",
      "Confusion matrix:\n",
      " [[1111616   27186]\n",
      " [  91915 1047831]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_true = []\n",
    "train_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        train_true.append(yb.cpu().numpy())\n",
    "        train_pred.append(preds.cpu().numpy())\n",
    "\n",
    "train_true = np.concatenate(train_true)\n",
    "train_pred = np.concatenate(train_pred)\n",
    "\n",
    "print(\"=== TRAIN METRICS ===\")\n",
    "print(\"Accuracy:\", accuracy_score(train_true, train_pred))\n",
    "print(\"Macro-F1:\", f1_score(train_true, train_pred, average=\"macro\"))\n",
    "print(\"Weighted-F1:\", f1_score(train_true, train_pred, average=\"weighted\"))\n",
    "print(\"Per-class F1:\", f1_score(train_true, train_pred, average=None))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(train_true, train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5838fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: True=1 Pred=1\n",
      "Sample 1: True=0 Pred=0\n",
      "Sample 2: True=1 Pred=1\n",
      "Sample 3: True=0 Pred=0\n",
      "Sample 4: True=0 Pred=0\n",
      "Sample 5: True=0 Pred=0\n",
      "Sample 6: True=1 Pred=1\n",
      "Sample 7: True=1 Pred=1\n",
      "Sample 8: True=1 Pred=0\n",
      "Sample 9: True=1 Pred=1\n",
      "Sample 10: True=0 Pred=0\n",
      "Sample 11: True=0 Pred=0\n",
      "Sample 12: True=1 Pred=0\n",
      "Sample 13: True=1 Pred=1\n",
      "Sample 14: True=1 Pred=1\n",
      "Sample 15: True=1 Pred=1\n",
      "Sample 16: True=0 Pred=0\n",
      "Sample 17: True=1 Pred=1\n",
      "Sample 18: True=1 Pred=1\n",
      "Sample 19: True=0 Pred=0\n",
      "Sample 20: True=1 Pred=1\n",
      "Sample 21: True=1 Pred=1\n",
      "Sample 22: True=1 Pred=1\n",
      "Sample 23: True=1 Pred=1\n",
      "Sample 24: True=0 Pred=0\n",
      "Sample 25: True=0 Pred=0\n",
      "Sample 26: True=1 Pred=1\n",
      "Sample 27: True=1 Pred=1\n",
      "Sample 28: True=1 Pred=1\n",
      "Sample 29: True=1 Pred=1\n",
      "Sample 30: True=0 Pred=0\n",
      "Sample 31: True=1 Pred=1\n",
      "Sample 32: True=1 Pred=1\n",
      "Sample 33: True=0 Pred=0\n",
      "Sample 34: True=1 Pred=1\n",
      "Sample 35: True=1 Pred=0\n",
      "Sample 36: True=1 Pred=1\n",
      "Sample 37: True=0 Pred=0\n",
      "Sample 38: True=0 Pred=0\n",
      "Sample 39: True=1 Pred=1\n",
      "Sample 40: True=1 Pred=1\n",
      "Sample 41: True=1 Pred=1\n",
      "Sample 42: True=1 Pred=1\n",
      "Sample 43: True=0 Pred=0\n",
      "Sample 44: True=1 Pred=1\n",
      "Sample 45: True=1 Pred=1\n",
      "Sample 46: True=1 Pred=1\n",
      "Sample 47: True=0 Pred=0\n",
      "Sample 48: True=1 Pred=1\n",
      "Sample 49: True=0 Pred=0\n",
      "Sample 50: True=0 Pred=0\n",
      "Sample 51: True=1 Pred=1\n",
      "Sample 52: True=0 Pred=0\n",
      "Sample 53: True=0 Pred=0\n",
      "Sample 54: True=1 Pred=0\n",
      "Sample 55: True=0 Pred=0\n",
      "Sample 56: True=0 Pred=0\n",
      "Sample 57: True=1 Pred=1\n",
      "Sample 58: True=0 Pred=0\n",
      "Sample 59: True=1 Pred=1\n",
      "Sample 60: True=1 Pred=1\n",
      "Sample 61: True=1 Pred=1\n",
      "Sample 62: True=0 Pred=0\n",
      "Sample 63: True=1 Pred=1\n",
      "Sample 64: True=0 Pred=0\n",
      "Sample 65: True=1 Pred=1\n",
      "Sample 66: True=1 Pred=1\n",
      "Sample 67: True=1 Pred=1\n",
      "Sample 68: True=1 Pred=1\n",
      "Sample 69: True=0 Pred=0\n",
      "Sample 70: True=0 Pred=0\n",
      "Sample 71: True=1 Pred=1\n",
      "Sample 72: True=0 Pred=0\n",
      "Sample 73: True=1 Pred=1\n",
      "Sample 74: True=1 Pred=1\n",
      "Sample 75: True=0 Pred=0\n",
      "Sample 76: True=1 Pred=1\n",
      "Sample 77: True=1 Pred=1\n",
      "Sample 78: True=0 Pred=0\n",
      "Sample 79: True=1 Pred=1\n",
      "Sample 80: True=1 Pred=1\n",
      "Sample 81: True=1 Pred=1\n",
      "Sample 82: True=1 Pred=1\n",
      "Sample 83: True=1 Pred=1\n",
      "Sample 84: True=1 Pred=1\n",
      "Sample 85: True=1 Pred=1\n",
      "Sample 86: True=1 Pred=1\n",
      "Sample 87: True=1 Pred=1\n",
      "Sample 88: True=1 Pred=1\n",
      "Sample 89: True=1 Pred=1\n",
      "Sample 90: True=1 Pred=1\n",
      "Sample 91: True=0 Pred=0\n",
      "Sample 92: True=0 Pred=0\n",
      "Sample 93: True=1 Pred=1\n",
      "Sample 94: True=1 Pred=1\n",
      "Sample 95: True=0 Pred=0\n",
      "Sample 96: True=1 Pred=1\n",
      "Sample 97: True=1 Pred=1\n",
      "Sample 98: True=0 Pred=0\n",
      "Sample 99: True=0 Pred=0\n",
      "Correct out of 100: 96\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(100):\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    sample = X_test[idx].reshape(1,-1)\n",
    "    true = y_test[idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(torch.from_numpy(sample).to(DEVICE))\n",
    "    pred = torch.argmax(out,1).item()\n",
    "\n",
    "    print(f\"Sample {i}: True={true} Pred={pred}\")\n",
    "    if pred == true: correct += 1\n",
    "\n",
    "print(\"Correct out of 100:\", correct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d18228b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Using model: model_kronnet_binary.pt\n",
      "\n",
      "[OK] Bundle ready at: D:/PARA/projects/FYP/IDS2/magnum_opus\\deployment_bundle\n",
      "manifest.json\n",
      "metrics_binary.json\n",
      "model_kronnet_binary.pt\n",
      "figures\\cm_test.png\n",
      "figures\\cm_train.png\n",
      "figures\\f1_train_vs_test.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_15168\\3223557973.py:113: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
      "C:\\Users\\HomePC\\AppData\\Local\\Temp\\ipykernel_15168\\3223557973.py:129: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"created_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n"
     ]
    }
   ],
   "source": [
    "# Build confusion matrices & F1 plots and create a clean deployment bundle\n",
    "# Uses your exact filenames from the folder screenshot.\n",
    "\n",
    "import os, json, shutil, datetime, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "ROOT = r\"D:/PARA/projects/FYP/IDS2/magnum_opus\"\n",
    "\n",
    "# Files exactly as in your folder\n",
    "NPZ_PATH   = os.path.join(ROOT, \"binary_balanced_tensors.npz\")\n",
    "MODEL_PT_5 = os.path.join(ROOT, \"model_kronnet_binary_ep5.pt\")  # may or may not exist\n",
    "MODEL_PT_1 = os.path.join(ROOT, \"model_kronnet_binary.pt\")      # exists per screenshot\n",
    "SCALER_PKL = os.path.join(ROOT, \"scaler.pkl\")                   # optional\n",
    "\n",
    "# Choose best available checkpoint (prefer 5-epoch)\n",
    "MODEL_PT = MODEL_PT_5 if os.path.exists(MODEL_PT_5) else MODEL_PT_1\n",
    "print(\"[info] Using model:\", os.path.basename(MODEL_PT))\n",
    "\n",
    "BUNDLE  = os.path.join(ROOT, \"deployment_bundle\")\n",
    "FIG_DIR = os.path.join(BUNDLE, \"figures\")\n",
    "os.makedirs(FIG_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Load tensors ----\n",
    "data = np.load(NPZ_PATH, allow_pickle=False)\n",
    "X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "X_val,   y_val   = data[\"X_val\"],   data[\"y_val\"]\n",
    "X_test,  y_test  = data[\"X_test\"],  data[\"y_test\"]\n",
    "\n",
    "in_dim = X_train.shape[1]\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- KronNet (same shape as training) ----\n",
    "class KronNetBinary(nn.Module):\n",
    "    def __init__(self, in_dim, p_drop=0.05):\n",
    "        super().__init__()\n",
    "        self.fc1  = nn.Linear(in_dim, 64)\n",
    "        self.fc2  = nn.Linear(64, 32)\n",
    "        self.map4 = nn.Linear(32, 4)   # -> (2,2)\n",
    "        self.L    = nn.Parameter(torch.eye(2))\n",
    "        self.head = nn.Linear(4, 2)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        for m in [self.fc1, self.fc2, self.map4, self.head]:\n",
    "            nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "    def forward(self, x):\n",
    "        x = self.drop(torch.relu(self.fc1(x)))\n",
    "        x = self.drop(torch.relu(self.fc2(x)))\n",
    "        x = self.map4(x).view(-1,2,2)\n",
    "        x = torch.matmul(self.L, x).view(-1,4)\n",
    "        return self.head(x)\n",
    "\n",
    "model = KronNetBinary(in_dim).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PT, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# ---- Eval helpers ----\n",
    "def eval_numpy(X, y, batch=8192):\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), batch):\n",
    "            xb = torch.from_numpy(X[i:i+batch]).to(torch.float32).to(DEVICE)\n",
    "            pred = torch.argmax(model(xb), dim=1).cpu().numpy()\n",
    "            y_pred.append(pred); y_true.append(y[i:i+batch])\n",
    "    y_true = np.concatenate(y_true); y_pred = np.concatenate(y_pred)\n",
    "    return {\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\", zero_division=0),\n",
    "        \"f1_per_class\": f1_score(y_true, y_pred, average=None, zero_division=0).tolist(),\n",
    "        \"cm\": confusion_matrix(y_true, y_pred, labels=[0,1]).tolist()\n",
    "    }\n",
    "\n",
    "def plot_cm(cm, title, path):\n",
    "    cm = np.array(cm)\n",
    "    fig, ax = plt.subplots(figsize=(4.8,4.2), dpi=140)\n",
    "    im = ax.imshow(cm, interpolation='nearest')\n",
    "    ax.set_title(title); ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "    ax.set_xticklabels(['Benign','Attack']); ax.set_yticklabels(['Benign','Attack'])\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, f\"{cm[i,j]:,}\", ha=\"center\", va=\"center\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout(); fig.savefig(path, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "def plot_f1(train_f1, test_f1, path):\n",
    "    labels = ['Benign','Attack']; x = np.arange(len(labels)); w = 0.35\n",
    "    fig, ax = plt.subplots(figsize=(5.2,3.8), dpi=140)\n",
    "    ax.bar(x - w/2, train_f1, w, label='Train'); ax.bar(x + w/2, test_f1, w, label='Test')\n",
    "    ax.set_ylim(0.0, 1.0); ax.set_xticks(x); ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel('F1-score'); ax.set_title('Per-class F1: Train vs Test'); ax.legend()\n",
    "    plt.tight_layout(); fig.savefig(path, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "# ---- Compute metrics ----\n",
    "train_rep = eval_numpy(X_train, y_train)\n",
    "val_rep   = eval_numpy(X_val,   y_val)\n",
    "test_rep  = eval_numpy(X_test,  y_test)\n",
    "\n",
    "# ---- Save figures ----\n",
    "plot_cm(train_rep[\"cm\"], \"Confusion Matrix (Train)\", os.path.join(FIG_DIR, \"cm_train.png\"))\n",
    "plot_cm(test_rep[\"cm\"],  \"Confusion Matrix (Test)\",  os.path.join(FIG_DIR, \"cm_test.png\"))\n",
    "plot_f1(train_rep[\"f1_per_class\"], test_rep[\"f1_per_class\"], os.path.join(FIG_DIR, \"f1_train_vs_test.png\"))\n",
    "\n",
    "# ---- Write fresh metrics JSON into bundle ----\n",
    "metrics_json_path = os.path.join(BUNDLE, \"metrics_binary.json\")\n",
    "with open(metrics_json_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"created_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"model_file_used\": os.path.basename(MODEL_PT),\n",
    "        \"train\": train_rep,\n",
    "        \"val\":   val_rep,\n",
    "        \"test\":  test_rep\n",
    "    }, f, indent=2)\n",
    "\n",
    "# ---- Copy artifacts ----\n",
    "shutil.copy2(MODEL_PT, BUNDLE)\n",
    "if os.path.exists(SCALER_PKL):\n",
    "    shutil.copy2(SCALER_PKL, BUNDLE)\n",
    "# If you also want to include tensors (large), uncomment:\n",
    "# shutil.copy2(NPZ_PATH, BUNDLE)\n",
    "\n",
    "# ---- Minimal manifest ----\n",
    "manifest = {\n",
    "    \"created_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"model_file\": os.path.basename(MODEL_PT),\n",
    "    \"metrics_file\": os.path.basename(metrics_json_path),\n",
    "    \"scaler_file\": os.path.basename(SCALER_PKL) if os.path.exists(SCALER_PKL) else None,\n",
    "    \"figures\": {\n",
    "        \"cm_train\": \"figures/cm_train.png\",\n",
    "        \"cm_test\":  \"figures/cm_test.png\",\n",
    "        \"f1_bars\":  \"figures/f1_train_vs_test.png\"\n",
    "    },\n",
    "    \"input_dim\": int(in_dim),\n",
    "    \"class_names\": [\"Benign\",\"Attack\"],\n",
    "    \"notes\": \"KronNet binary IDS (64→32→4→2). Check metrics_binary.json for scores.\"\n",
    "}\n",
    "with open(os.path.join(BUNDLE, \"manifest.json\"), \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(\"\\n[OK] Bundle ready at:\", BUNDLE)\n",
    "for root, _, files in os.walk(BUNDLE):\n",
    "    for name in files:\n",
    "        print(os.path.relpath(os.path.join(root, name), BUNDLE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c7124c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'SSH', 'TCP', 'UDP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label', 'label_bin']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_clean_binary.csv.gz\", nrows=5)\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e9d8ff30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] scaler.pkl saved to: D:/PARA/projects/FYP/IDS2/magnum_opus\\scaler.pkl\n",
      "TRAIN samples used for stats: 2,278,548\n",
      "Per-label train rows used (top few):\n",
      "{'BenignTraffic': 700000, 'DDoS-ACK_Fragmentation': 70000, 'DDoS-HTTP_Flood': 20153, 'DDoS-ICMP_Flood': 70000, 'DDoS-ICMP_Fragmentation': 70000}\n"
     ]
    }
   ],
   "source": [
    "# Rebuild StandardScaler (train-only) from combined_clean_binary.csv, memory-safe\n",
    "# - reproduces the same balanced selection you used for tensors\n",
    "# - assigns rows into 70/15/15 per-class quotas in a streaming manner\n",
    "# - fits scaler ONLY on TRAIN rows via Welford running stats\n",
    "# - saves scaler.pkl for deployment\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ROOT = r\"D:/PARA/projects/FYP/IDS2/magnum_opus\"\n",
    "SRC  = os.path.join(ROOT, \"combined_clean_binary.csv.gz\")   # your file as in screenshot\n",
    "OUT  = os.path.join(ROOT, \"scaler.pkl\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Feature columns in correct order (your list)\n",
    "FEATURE_COLS = [\n",
    " 'flow_duration','Header_Length','Protocol Type','Duration','Rate','Srate','Drate',\n",
    " 'fin_flag_number','syn_flag_number','rst_flag_number','psh_flag_number','ack_flag_number',\n",
    " 'ack_count','syn_count','fin_count','urg_count','rst_count','HTTP','HTTPS','DNS',\n",
    " 'SSH','TCP','UDP','ARP','ICMP','IPv','LLC','Tot sum','Min','Max','AVG','Std',\n",
    " 'Tot size','IAT','Number','Magnitue','Radius','Covariance','Variance','Weight'\n",
    "]\n",
    "TARGET_COL  = \"label\"\n",
    "BIN_COL     = \"label_bin\"\n",
    "\n",
    "# Balance settings you used\n",
    "BENIGN_NAME = \"BenignTraffic\"\n",
    "BENIGN_CAP  = 1_000_000\n",
    "ATTACK_CAP  = 100_000\n",
    "MIN_KEEP    = 500\n",
    "DROP_ATTACKS = {\n",
    "    \"BrowserHijacking\",\"CommandInjection\",\"SqlInjection\",\"XSS\",\n",
    "    \"Backdoor_Malware\",\"Recon-PingSweep\",\"Uploading_Attack\"\n",
    "}\n",
    "\n",
    "CHUNK = 500_000\n",
    "\n",
    "# ---------- PASS 1: count labels to compute quotas ----------\n",
    "label_counts = Counter()\n",
    "for chunk in pd.read_csv(SRC, usecols=[TARGET_COL], chunksize=CHUNK, low_memory=False):\n",
    "    label_counts.update(chunk[TARGET_COL].astype(str))\n",
    "\n",
    "benign_total = label_counts.get(BENIGN_NAME, 0)\n",
    "attack_labels = [k for k in label_counts if k != BENIGN_NAME]\n",
    "\n",
    "quotas_total = {}  # total rows to keep per label (after caps/drops)\n",
    "for lab in attack_labels:\n",
    "    cnt = label_counts[lab]\n",
    "    if lab in DROP_ATTACKS or cnt < MIN_KEEP:\n",
    "        continue\n",
    "    quotas_total[lab] = min(cnt, ATTACK_CAP)\n",
    "\n",
    "if benign_total > 0:\n",
    "    quotas_total[BENIGN_NAME] = min(benign_total, BENIGN_CAP)\n",
    "\n",
    "# split quotas: 70/15/15 per label\n",
    "def split_quota(n):\n",
    "    tr = int(round(n * 0.70))\n",
    "    va = int(round(n * 0.15))\n",
    "    te = n - tr - va\n",
    "    return tr, va, te\n",
    "\n",
    "split_need = {}  # label -> {\"train\": int, \"val\": int, \"test\": int}\n",
    "for lab, n in quotas_total.items():\n",
    "    tr, va, te = split_quota(n)\n",
    "    split_need[lab] = {\"train\": tr, \"val\": va, \"test\": te}\n",
    "\n",
    "# ---------- Welford running stats for TRAIN ONLY ----------\n",
    "D = len(FEATURE_COLS)\n",
    "count = 0\n",
    "mean  = np.zeros(D, dtype=np.float64)\n",
    "M2    = np.zeros(D, dtype=np.float64)\n",
    "\n",
    "def update_welford(Xbatch):\n",
    "    global count, mean, M2\n",
    "    # Xbatch: (B, D) float\n",
    "    for x in Xbatch:\n",
    "        count += 1\n",
    "        delta = x - mean\n",
    "        mean += delta / count\n",
    "        delta2 = x - mean\n",
    "        M2 += delta * delta2\n",
    "\n",
    "# ---------- PASS 2: stream rows, assign to splits, update scaler stats on TRAIN ----------\n",
    "kept_counters = defaultdict(lambda: {\"train\":0, \"val\":0, \"test\":0})\n",
    "total_seen = 0\n",
    "\n",
    "for chunk in pd.read_csv(SRC, chunksize=CHUNK, low_memory=False):\n",
    "    # enforce dtypes\n",
    "    sub = chunk[[*FEATURE_COLS, TARGET_COL, BIN_COL]].copy()\n",
    "    sub[TARGET_COL] = sub[TARGET_COL].astype(str)\n",
    "\n",
    "    # filter to labels we actually keep\n",
    "    mask_keep = sub[TARGET_COL].isin(quotas_total.keys())\n",
    "    if not mask_keep.any():\n",
    "        continue\n",
    "    sub = sub[mask_keep]\n",
    "\n",
    "    # iterate by label for simple quota assignment\n",
    "    for lab, grp in sub.groupby(TARGET_COL):\n",
    "        need = split_need.get(lab, None)\n",
    "        if not need:\n",
    "            continue\n",
    "        # compute remaining per split\n",
    "        rem_tr = need[\"train\"] - kept_counters[lab][\"train\"]\n",
    "        rem_va = need[\"val\"]   - kept_counters[lab][\"val\"]\n",
    "        rem_te = need[\"test\"]  - kept_counters[lab][\"test\"]\n",
    "        if rem_tr <= 0 and rem_va <= 0 and rem_te <= 0:\n",
    "            continue\n",
    "\n",
    "        # take as many as needed for each split in order: train -> val -> test\n",
    "        take_tr = max(0, min(rem_tr, len(grp)))\n",
    "        if take_tr > 0:\n",
    "            X_tr = grp.iloc[:take_tr][FEATURE_COLS].to_numpy(np.float64, copy=False)\n",
    "            update_welford(X_tr)\n",
    "            kept_counters[lab][\"train\"] += take_tr\n",
    "            grp = grp.iloc[take_tr:]\n",
    "\n",
    "        # (we skip collecting val/test rows; we only need train rows for scaler)\n",
    "\n",
    "        # early exit if all quotas for this label are done\n",
    "        # Note: speeds up overall\n",
    "        # (Remaining val/test quotas will be satisfied by later chunks, but not needed for scaler.)\n",
    "\n",
    "    total_seen += len(sub)\n",
    "\n",
    "# finalize stats\n",
    "if count == 0:\n",
    "    raise RuntimeError(\"No TRAIN rows were processed for scaler fitting. Check quotas/file paths.\")\n",
    "\n",
    "var = M2 / max(count - 1, 1)\n",
    "scale = np.sqrt(np.maximum(var, 1e-12))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# setattr the learned parameters explicitly\n",
    "scaler.mean_ = mean.astype(np.float64)\n",
    "scaler.var_  = var.astype(np.float64)\n",
    "scaler.scale_= scale.astype(np.float64)\n",
    "scaler.n_samples_seen_ = np.array([count], dtype=np.int64)\n",
    "\n",
    "# tiny sanity test on a small batch\n",
    "test_batch = pd.read_csv(SRC, nrows=10, low_memory=False)[FEATURE_COLS].to_numpy(np.float64)\n",
    "_ = scaler.transform(test_batch)  # should run without error and keep shape\n",
    "\n",
    "joblib.dump(scaler, OUT)\n",
    "print(f\"[OK] scaler.pkl saved to: {OUT}\")\n",
    "print(f\"TRAIN samples used for stats: {count:,}\")\n",
    "print(\"Per-label train rows used (top few):\")\n",
    "print({k: v['train'] for k, v in list(kept_counters.items())[:5]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd27bbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns:\n",
      "['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'SSH', 'TCP', 'UDP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight', 'label', 'label_bin']\n",
      "\n",
      "Feature columns (X):\n",
      "['flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'SSH', 'TCP', 'UDP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight']\n",
      "\n",
      "Number of features: 40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = r\"D:/PARA/projects/FYP/IDS2/magnum_opus/combined_clean_binary.csv.gz\"\n",
    "\n",
    "# Read only header (no data loaded)\n",
    "cols = pd.read_csv(FILE, nrows=0).columns.tolist()\n",
    "\n",
    "print(\"All columns:\")\n",
    "print(cols)\n",
    "\n",
    "print(\"\\nFeature columns (X):\")\n",
    "feature_cols = [c for c in cols if c not in [\"label\", \"label_bin\"]]\n",
    "print(feature_cols)\n",
    "\n",
    "print(\"\\nNumber of features:\", len(feature_cols))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a037d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
